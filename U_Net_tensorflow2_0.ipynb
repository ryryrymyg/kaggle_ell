{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net_tensorflow2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJn-8Feqw7YK"
      },
      "source": [
        "# Tensorflow2.0によるU-Netの実装\n",
        "この[ISSUE](https://github.com/ryryrymyg/kaggle_ell/issues/31)で示したU-NetをTensor Flow 2.0で実装する。\n",
        "\n",
        "参照元: https://qiita.com/hiro871_/items/871c76bf65b76ebe1dd0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW4fYMMw4ykq"
      },
      "source": [
        "## 1. ライブラリのインポート\n",
        "必要なライブラリをインポートする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vz9aOUAwcet"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, Dropout, Flatten, Dense"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGa50bfn5I15"
      },
      "source": [
        "## 2. モデル、損失関数、オプティマイザの設定\n",
        "画像セグメンテーションでは損失関数として[Dice係数](https://mieruca-ai.com/ai/jaccard_dice_simpson/)、[SparseCategoricalCrossentropy](https://runebook.dev/ja/docs/tensorflow/keras/losses/sparsecategoricalcrossentropy)(ラベルと予測値の間の交差エントロピー損失を計算する)等を利用することができる。\n",
        "損失関数は[BinaryCrossentropy](https://yaakublog.com/crossentropy_binarycrossentropy)を使用し、[オプティマイザ](https://qiita.com/omiita/items/1735c1d048fe5f611f80)はAdamを使った。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7RhpRrX1fdz"
      },
      "source": [
        "# kerasのModelクラスを継承したUNetクラスの作成\n",
        "class UNet(Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Network\n",
        "        # Encoder(後述)とDecoder(後述)を定義する\n",
        "        self.enc = Encoder(config)\n",
        "        self.dec = Decoder(config)\n",
        "\n",
        "        # Optimizer\n",
        "        # オプティマイザをAdamとする\n",
        "        self.optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "        # loss\n",
        "        # 損失関数の定義\n",
        "        self.loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "        self.train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32) #重み付き平均出すメソッド\n",
        "        self.valid_loss = tf.keras.metrics.Mean('valid_loss', dtype=tf.float32) #重み付き平均出すメソッド\n",
        "\n",
        "    # エンコーダにオブジェクトxを代入して出力yを出す\n",
        "    def call(self, x):\n",
        "        z1, z2, z3, z4_dropout, z5_dropout = self.enc(x)\n",
        "        y = self.dec(z1, z2, z3, z4_dropout, z5_dropout)\n",
        "\n",
        "        return y\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x, t):\n",
        "        with tf.GradientTape() as tape: # tf.GradientTape(): テープに演算全てを記録。その後トップダウン型自動微分を用いて演算それぞれに対する勾配を計算する\n",
        "            y = self.call(x)\n",
        "            loss = self.loss_object(t, y) # 損失関数にラベルと予測結果をぶち込む\n",
        "        gradients = tape.gradient(loss, self.trainable_variables) # self.trainable_variablesに対するlossの微分\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) #オプティマイザに勾配とtrainable_variablesをzipして格納\n",
        "        self.train_loss(loss)    # lossの平均をとる\n",
        "\n",
        "    @tf.function\n",
        "    def valid_step(self, x, t):\n",
        "        y = self.call(x)\n",
        "        v_loss = self.loss_object(t, y)\n",
        "        self.valid_loss(v_loss) # 勾配とってない\n",
        "\n",
        "        return y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVAMBwkbNT_V"
      },
      "source": [
        "## 3. Encoderの定義\n",
        "U-NetのEncoderの特徴は下記の通り。\n",
        "\n",
        "典型的なConvolution network\n",
        "1. 3X3 convolutionを二回反復して行う\n",
        "2. 活性化関数でReLUを使う\n",
        "3. 2X2 max pooling と stride 2を使う\n",
        "4. downsampling時、 2倍のfeature channelを利用する\n",
        "\n",
        "これらの特徴を元に実装して行くと、このようになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ThrRMvItSV"
      },
      "source": [
        "class Encoder(Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Network\n",
        "        # 3X3 convolutionを二回反復します。\n",
        "        self.block1_conv1 = tf.keras.layers.Conv2D(64, (3, 3) , name='block1_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block1_conv2 = tf.keras.layers.Conv2D(64, (3, 3) , name='block1_conv2', padding = 'same')\n",
        "        self.block1_bn = tf.keras.layers.BatchNormalization()\n",
        "        # 活性化関数でReLUを使います。\n",
        "        self.block1_act = tf.keras.layers.ReLU()\n",
        "        # 2X2 max pooling と stride 2を使います。\n",
        "        self.block1_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None, name='block1_pool')\n",
        "\n",
        "        # 以下繰り返し\n",
        "        self.block2_conv1 = tf.keras.layers.Conv2D(128, (3, 3) , name='block2_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block2_conv2 = tf.keras.layers.Conv2D(128, (3, 3) , name='block2_conv2', padding = 'same')\n",
        "        self.block2_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block2_act = tf.keras.layers.ReLU()\n",
        "        self.block2_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None, name='block2_pool')\n",
        "\n",
        "        self.block3_conv1 = tf.keras.layers.Conv2D(256, (3, 3) , name='block3_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block3_conv2 = tf.keras.layers.Conv2D(256, (3, 3) , name='block3_conv2', padding = 'same')\n",
        "        self.block3_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block3_act = tf.keras.layers.ReLU()\n",
        "        self.block3_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None, name='block3_pool')\n",
        "\n",
        "        self.block4_conv1 = tf.keras.layers.Conv2D(512, (3, 3) , name='block4_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block4_conv2 = tf.keras.layers.Conv2D(512, (3, 3) , name='block4_conv2', padding = 'same')\n",
        "        self.block4_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block4_act = tf.keras.layers.ReLU()\n",
        "        self.block4_dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.block4_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None, name='block4_pool')\n",
        "\n",
        "        self.block5_conv1 = tf.keras.layers.Conv2D(1024, (3, 3) , name='block5_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block5_conv2 = tf.keras.layers.Conv2D(1024, (3, 3) , name='block5_conv2', padding = 'same')\n",
        "        self.block5_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block5_act = tf.keras.layers.ReLU()\n",
        "        self.block5_dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, x):\n",
        "        z1 = self.block1_conv1(x)\n",
        "        z1 = self.block1_conv2(z1)\n",
        "        z1 = self.block1_bn(z1)\n",
        "        z1 = self.block1_act(z1)\n",
        "        z1_pool = self.block1_pool(z1)\n",
        "\n",
        "        z2 = self.block2_conv1(z1_pool)\n",
        "        z2 = self.block2_conv2(z2)\n",
        "        z2 = self.block2_bn(z2)\n",
        "        z2 = self.block2_act(z2)\n",
        "        z2_pool = self.block2_pool(z2)\n",
        "\n",
        "        z3 = self.block3_conv1(z2_pool)\n",
        "        z3 = self.block3_conv2(z3)\n",
        "        z3 = self.block3_bn(z3)\n",
        "        z3 = self.block3_act(z3)\n",
        "        z3_pool = self.block3_pool(z3)\n",
        "\n",
        "        z4 = self.block4_conv1(z3_pool)\n",
        "        z4 = self.block4_conv2(z4)\n",
        "        z4 = self.block4_bn(z4)\n",
        "        z4 = self.block4_act(z4)\n",
        "        z4_dropout = self.block4_dropout(z4)\n",
        "        z4_pool = self.block4_pool(z4_dropout)\n",
        "\n",
        "        z5 = self.block5_conv1(z4_pool)\n",
        "        z5 = self.block5_conv2(z5)\n",
        "        z5 = self.block5_bn(z5)\n",
        "        z5 = self.block5_act(z5)\n",
        "        z5_dropout = self.block5_dropout(z5)\n",
        "\n",
        "        return z1, z2, z3, z4_dropout, z5_dropout\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGlmjSdyNz2Q"
      },
      "source": [
        "## 4. Decoderの定義\n",
        "U-NetのDecoderの特徴は下記の通り。\n",
        "1. 2X2 convolution (up-convolution)を使う\n",
        "2. feature channelは半分で 減らして使用する\n",
        "3. EncoderでMax-Poolingする前のfeature mapをCropして、Up-Convolutionする時concatenation(連結)する\n",
        "4. 3X3 convolutionを二回反復して行う\n",
        "5. 活性化関数でReLUを使う\n",
        "6. 最後のレイヤーでは 1X1 convolutionを使って2個のクラスで分類する\n",
        "\n",
        "これらの特徴を元に実装して行くとこのようになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPkK3wFIOSz1"
      },
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Network\n",
        "        self.block6_up = tf.keras.layers.UpSampling2D(size = (2,2))\n",
        "        #  2X2 convolution (up-convolution)を使います。\n",
        "        self.block6_conv1 = tf.keras.layers.Conv2D(512, (2, 2) , name='block6_conv1', activation = 'relu', padding = 'same')\n",
        "        # 3X3 convolutionを二回反復して行います。\n",
        "        self.block6_conv2 = tf.keras.layers.Conv2D(512, (3, 3) , name='block6_conv2', activation = 'relu', padding = 'same')\n",
        "        self.block6_conv3 = tf.keras.layers.Conv2D(512, (3, 3) , name='block6_conv3', padding = 'same')\n",
        "        self.block6_bn = tf.keras.layers.BatchNormalization()\n",
        "        # 活性化関数でReLUを使います。\n",
        "        self.block6_act = tf.keras.layers.ReLU()\n",
        "\n",
        "        self.block7_up = tf.keras.layers.UpSampling2D(size = (2,2))\n",
        "        # feature channelは前の層より半分で 減らして使用します。\n",
        "        self.block7_conv1 = tf.keras.layers.Conv2D(256, (2, 2) , name='block7_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block7_conv2 = tf.keras.layers.Conv2D(256, (3, 3) , name='block7_conv2', activation = 'relu', padding = 'same')\n",
        "        self.block7_conv3 = tf.keras.layers.Conv2D(256, (3, 3) , name='block7_conv3', padding = 'same')\n",
        "        self.block7_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block7_act = tf.keras.layers.ReLU()\n",
        "\n",
        "        self.block8_up = tf.keras.layers.UpSampling2D(size = (2,2))\n",
        "        self.block8_conv1 = tf.keras.layers.Conv2D(128, (2, 2) , name='block8_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block8_conv2 = tf.keras.layers.Conv2D(128, (3, 3) , name='block8_conv2', activation = 'relu', padding = 'same')\n",
        "        self.block8_conv3 = tf.keras.layers.Conv2D(128, (3, 3) , name='block8_conv3', padding = 'same')\n",
        "        self.block8_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block8_act = tf.keras.layers.ReLU()\n",
        "\n",
        "        self.block9_up = tf.keras.layers.UpSampling2D(size = (2,2))\n",
        "        self.block9_conv1 = tf.keras.layers.Conv2D(64, (2, 2) , name='block9_conv1', activation = 'relu', padding = 'same')\n",
        "        self.block9_conv2 = tf.keras.layers.Conv2D(64, (3, 3) , name='block9_conv2', activation = 'relu', padding = 'same')\n",
        "        self.block9_conv3 = tf.keras.layers.Conv2D(64, (3, 3) , name='block9_conv3', padding = 'same')\n",
        "        self.block9_bn = tf.keras.layers.BatchNormalization()\n",
        "        self.block9_act = tf.keras.layers.ReLU()\n",
        "        #  最後のレイヤーでは 1X1 convolutionを使って2個のクラスで分類します。\n",
        "        self.output_conv = tf.keras.layers.Conv2D(config.model.num_class, (1, 1), name='output_conv', activation = 'sigmoid')\n",
        "\n",
        "    def call(self, z1, z2, z3, z4_dropout, z5_dropout):\n",
        "        z6_up = self.block6_up(z5_dropout)\n",
        "        z6 = self.block6_conv1(z6_up)\n",
        "        # EncoderでMax-Poolingする前のfeature mapをCropして、Up-Convolutionする時concatenationします。\n",
        "        z6 = tf.keras.layers.concatenate([z4_dropout,z6], axis = 3)\n",
        "        z6 = self.block6_conv2(z6)\n",
        "        z6 = self.block6_conv3(z6)\n",
        "        z6 = self.block6_bn(z6)\n",
        "        z6 = self.block6_act(z6)\n",
        "\n",
        "        z7_up = self.block7_up(z6)\n",
        "        z7 = self.block7_conv1(z7_up)\n",
        "        z7 = tf.keras.layers.concatenate([z3, z7], axis = 3)\n",
        "        z7 = self.block7_conv2(z7)\n",
        "        z7 = self.block7_conv3(z7)\n",
        "        z7 = self.block7_bn(z7)\n",
        "        z7 = self.block7_act(z7)\n",
        "\n",
        "        z8_up = self.block8_up(z7)\n",
        "        z8 = self.block8_conv1(z8_up)\n",
        "        z8 = tf.keras.layers.concatenate([z2, z8], axis = 3)\n",
        "        z8 = self.block8_conv2(z8)\n",
        "        z8 = self.block8_conv3(z8)\n",
        "        z8 = self.block8_bn(z8)\n",
        "        z8 = self.block8_act(z8)\n",
        "\n",
        "        z9_up = self.block9_up(z8)\n",
        "        z9 = self.block9_conv1(z9_up)\n",
        "        z9 = tf.keras.layers.concatenate([z1, z9], axis = 3)\n",
        "        z9 = self.block9_conv2(z9)\n",
        "        z9 = self.block9_conv3(z9)\n",
        "        z9 = self.block9_bn(z9)\n",
        "        z9 = self.block9_act(z9)\n",
        "        y = self.output_conv(z9)\n",
        "\n",
        "        return y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvy37l5HOYvE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}